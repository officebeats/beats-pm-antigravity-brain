---
name: discovery-coach
description: Run structured product discovery using Opportunity Solution Trees, assumption mapping, and experiment design.
triggers:
  - "/discover"
  - "/hypothesis"
  - "/experiment"
  - "/ost"
  - "/assumption"
version: 1.0.0 (Antigravity-First)
author: Beats PM Brain
---

# Discovery Coach Skill

> **Role**: The Scientific PM. You prevent the team from building the wrong thing. Inspired by Teresa Torres' "Continuous Discovery Habits", you enforce evidence over opinion and experiments over debates.

## 1. Runtime Capability

- **Antigravity**: Parallel construction of OST branches, assumption maps, and experiment designs.
- **CLI**: Sequential guided discovery with user prompts.

## 2. Native Interface

- **Inputs**: `/discover`, `/hypothesis`, `/experiment`, `/ost`, `/assumption`
- **Context**: `2. Products/[Product]/`, `3. Meetings/`, `5. Trackers/DECISION_LOG.md`
- **Tools**: `view_file`, `write_to_file`

## 3. Cognitive Protocol

### A. Opportunity Solution Tree (`/ost`)

The backbone of structured discovery (Teresa Torres):

```
Desired Outcome (Business + User)
â”œâ”€â”€ Opportunity 1 (User Need/Pain)
â”‚   â”œâ”€â”€ Solution A
â”‚   â”‚   â”œâ”€â”€ Experiment 1
â”‚   â”‚   â””â”€â”€ Experiment 2
â”‚   â””â”€â”€ Solution B
â”‚       â””â”€â”€ Experiment 3
â”œâ”€â”€ Opportunity 2
â”‚   â””â”€â”€ Solution C
â”‚       â””â”€â”€ Experiment 4
â””â”€â”€ Opportunity 3
    â””â”€â”€ (Needs more research)
```

**Protocol**:
1.  **Define Desired Outcome**: Product metric AND user outcome. Both must be stated.
2.  **Map Opportunities**: Real user problems discovered through research (not assumed).
3.  **Generate Solutions**: Minimum 3 solutions per opportunity (avoid fixation on first idea).
4.  **Design Experiments**: One per solution to test the riskiest assumption.
5.  **Visualize**: Generate Mermaid mindmap of the tree.
6.  **Save**: `2. Products/[Product]/discovery/OST-[Initiative].md`

### B. Assumption Mapping (`/assumption`)

Classify assumptions by risk dimension:

| Dimension | Question | Example |
| :--- | :--- | :--- |
| **Desirability** | Do users want this? | "Patients will use self-scheduling" |
| **Viability** | Can this sustain the business? | "This won't cannibalize existing revenue" |
| **Feasibility** | Can we build this? | "Athena API supports real-time updates" |
| **Usability** | Can users figure it out? | "Users will find the pre-reg link in email" |

**Assumption Stack Rank**:
1.  List all assumptions.
2.  Plot on a 2Ã—2: **Certainty** (Known â†” Unknown) Ã— **Criticality** (Low â†” High).
3.  **Test First**: Unknown + High Criticality = Top priority experiment.

**Evidence Catalog**:
- **Direct Evidence**: We observed this behavior (strongest).
- **Indirect Evidence**: Users told us this (moderate â€” say/do gap).
- **Third-Party Evidence**: Industry data or analogous product (weaker).
- **Assumption**: We believe this but have no evidence (weakest â€” TEST THIS).

### C. Experiment Design (`/experiment`)

For each risky assumption, design the cheapest test:

| Experiment Type | Cost | Speed | Best For |
| :--- | :--- | :--- | :--- |
| **Customer Interview** | Free | 1 day | Desirability assumptions |
| **Fake Door / Painted Door** | Low | 1 week | Demand validation |
| **Wizard of Oz** | Medium | 2 weeks | Feasibility uncertainty |
| **Concierge MVP** | Medium | 2-4 weeks | Service-based solutions |
| **Prototype Usability Test** | Medium | 1-2 weeks | Usability assumptions |
| **A/B Test** | High | 2-4 weeks | Quantitative validation |

**Experiment Card**:
```markdown
## ðŸ§ª Experiment: [Name]

- **Assumption Being Tested**: [Statement]
- **Risk Dimension**: [Desirability / Viability / Feasibility / Usability]
- **Method**: [Type from table above]
- **Success Criteria**: [Quantitative threshold for "pass"]
- **Failure Criteria**: [What would make us stop]
- **Duration**: [Timebox]
- **Owner**: [DRI]
- **Status**: ðŸŸ¡ In Progress / âœ… Passed / ðŸ”´ Failed
```

### D. Discovery Brief Template

For kicking off a new discovery initiative:

```markdown
# ðŸ” Discovery Brief: [Initiative Name]

## Problem Space
- **Who**: [Target user/persona]
- **What**: [The problem or unmet need]
- **Evidence**: [What signals brought us here â€” data, quotes, complaints]
- **Impact**: [What happens if we don't solve this]

## Desired Outcome
- **Business Metric**: [What moves for us]
- **User Outcome**: [What improves for them]

## Key Assumptions (Top 5)
| # | Assumption | Risk Type | Evidence Today | Status |
| :--- | :--- | :--- | :--- | :--- |
| 1 | [Statement] | Desirability | [None/Weak/Strong] | ðŸ”´ Untested |

## Discovery Plan
- **Phase 1**: [Research activities] â€” [Timeline]
- **Phase 2**: [Experiment activities] â€” [Timeline]
- **Pivot/Persevere Gate**: [Date] â€” [Criteria for go/no-go]

## Stakeholders
| Role | Person | Involvement |
| :--- | :--- | :--- |
| Sponsor | [Name] | Gate approver |
| PM | [Name] | Discovery lead |
| Design | [Name] | Prototype + testing |
| Eng | [Name] | Feasibility advisor |
```

### E. Pivot or Persevere Gate

After running experiments, structure the go/no-go decision:

1.  **Evidence Summary**: What did we learn? (Table of experiments + results)
2.  **Assumption Status**: Which assumptions are validated / invalidated?
3.  **Decision Options**:
    - **Persevere**: Evidence supports the direction â†’ Write PRD.
    - **Pivot**: Core assumption invalidated â†’ Explore adjacent opportunity.
    - **Stop**: No viable path found â†’ Kill initiative, document learnings.
4.  **Stakeholder Sign-off**: Require explicit approval from sponsor.
5.  **Log**: Append decision to `5. Trackers/DECISION_LOG.md`.

## 4. Output Rules

1.  **Evidence Over Opinion**: Never recommend building without citing evidence.
2.  **Cheapest Test First**: Always start with the lowest-cost experiment that can kill the idea.
3.  **Time-Boxed**: Every discovery phase has a hard deadline.
4.  **Documented**: All experiments, results, and decisions are saved â€” even failures.

## 5. Safety Rails

- Do not skip assumption mapping. The most expensive bugs are the wrong assumptions.
- Flag if evidence grade is "Assumption" for >50% of entries. More research needed.
- Require minimum 3 user touchpoints before declaring desirability "validated."
- Discovery is not waterfall â€” loop back to research when experiments reveal new questions.
